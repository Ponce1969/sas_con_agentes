# backend/app/infrastructure/gemini_client.py

import os
import httpx
from typing import List, Dict, Any, Optional
import asyncio
import logging

logger = logging.getLogger(__name__)

class GeminiClient:
    """
    Cliente para la API de Google Gemini.
    Soporta: gemini-1.5-flash, gemini-2.0-flash-exp, gemini-2.5-flash (futuro)
    Maneja llamadas async y API key desde .env.
    """

    def __init__(
        self, 
        api_key: Optional[str] = None, 
        base_url: str = "https://generativelanguage.googleapis.com/v1beta"
    ):
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("Se requiere GEMINI_API_KEY para usar GeminiClient")
        self.base_url = base_url
        self.headers = {
            "Content-Type": "application/json",
        }

    async def create_embedding(self, text: str, model: str = "gemini-text-004") -> List[float]:
        """
        Solicita un embedding para un texto dado.
        """
        url = f"{self.base_url}/embeddings"
        payload = {
            "model": model,
            "input": text
        }
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                resp = await client.post(url, headers=self.headers, json=payload)
                resp.raise_for_status()
                data = resp.json()
                embedding = data.get("embedding")
                if embedding is None:
                    logger.warning(f"No se recibi√≥ embedding: {data}")
                    return []
                return embedding
            except httpx.HTTPStatusError as e:
                logger.error(f"HTTP error al generar embedding: {e.response.text}")
                return []
            except Exception as e:
                logger.error(f"Error inesperado al generar embedding: {e}")
                return []

    async def create_embeddings_batch(self, texts: List[str], model: str = "gemini-text-004") -> Dict[str, List[float]]:
        """
        Solicita embeddings para un batch de textos de forma concurrente.
        """
        tasks = [self.create_embedding(text, model=model) for text in texts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        embeddings = {}
        for i, res in enumerate(results):
            if isinstance(res, Exception):
                logger.error(f"Error embedding para '{texts[i]}': {res}")
                embeddings[texts[i]] = []
            else:
                embeddings[texts[i]] = res
        return embeddings

    async def analyze_code(self, code: str, model: str = "gemini-1.5-flash") -> str:
        """
        Analiza c√≥digo Python y retorna sugerencias de mejora.
        
        Args:
            code: C√≥digo Python a analizar
            model: Modelo de Gemini a usar (gemini-1.5-flash, gemini-2.0-flash-exp, gemini-2.5-flash)
            
        Returns:
            An√°lisis en formato markdown
        """
        prompt = f"""Eres un experto en Python con 10 a√±os de experiencia. Analiza este c√≥digo y proporciona:

1. **üêõ Bugs Potenciales**: Errores que podr√≠an causar problemas en producci√≥n
2. **üëÉ Code Smells**: Malas pr√°cticas o c√≥digo que "huele mal"
3. **‚ö° Mejoras de Rendimiento**: Optimizaciones posibles
4. **üìä Score de Calidad**: Calificaci√≥n de 0-100 con justificaci√≥n

C√≥digo a analizar:
```python
{code}
```

Formato de respuesta en Markdown:
## üêõ Bugs Potenciales
- [lista de bugs o "No se detectaron bugs"]

## üëÉ Code Smells
- [lista de code smells o "C√≥digo limpio"]

## ‚ö° Mejoras de Rendimiento
- [lista de mejoras o "Rendimiento √≥ptimo"]

## üìä Score de Calidad: [0-100]
[justificaci√≥n del score en 2-3 l√≠neas]

S√© espec√≠fico, constructivo y profesional."""

        # URL de la API oficial de Google Gemini
        url = f"{self.base_url}/models/{model}:generateContent?key={self.api_key}"
        
        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "temperature": 0.3,
                "maxOutputTokens": 2048,
                "topP": 0.8,
                "topK": 10
            }
        }
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                resp = await client.post(url, headers=self.headers, json=payload)
                resp.raise_for_status()
                data = resp.json()
                
                # Extraer texto de la respuesta de Gemini
                if "candidates" in data and len(data["candidates"]) > 0:
                    candidate = data["candidates"][0]
                    if "content" in candidate and "parts" in candidate["content"]:
                        parts = candidate["content"]["parts"]
                        if len(parts) > 0 and "text" in parts[0]:
                            analysis = parts[0]["text"]
                            return analysis
                
                logger.warning(f"No se recibi√≥ an√°lisis en formato esperado: {data}")
                return "‚ö†Ô∏è No se pudo generar el an√°lisis"
                
            except httpx.HTTPStatusError as e:
                logger.error(f"HTTP error al analizar c√≥digo: {e.response.text}")
                return f"‚ùå Error HTTP: {e.response.status_code}\n\nDetalles: {e.response.text[:200]}"
            except Exception as e:
                logger.error(f"Error inesperado al analizar c√≥digo: {e}")
                return f"‚ùå Error: {str(e)}"

# ----------------- USO EJEMPLO -----------------
# async def main():
#     client = GeminiClient()
#     code = "def suma(a, b): return a + b"
#     analysis = await client.analyze_code(code)
#     print(analysis)
# asyncio.run(main())

